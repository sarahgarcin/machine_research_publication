{
    "zoom": 0.8599999999999997,
    "imgPosX": 0,
    "imgPosY": 0,
    "longPosX": 2,
    "longPosY": 1,
    "shortPosX": 0,
    "shortPosY": 11,
    "space": 0,
    "txtlong": "This post has been written in relation to, and as a subset of, a body of work –an ‘ethnography of ethics‘ – that follows the emergence of the driverless car in Europe and North America. An ethnography of ethics is an acknowledgment of the need for a “thick” reading of what ethics means – and does not mean – in the context of big data: how it is constituted in relation to, and by, social, economic, political and technical forces; how it is put to work; and what its place is in a moment when autonomous vehicles and artificially intelligent computing receive significant interest and support. I argue that ethics is not necessarily an end-point or outcome, but is a series of individual and system-level negotiations involving socio-technical, technical, human and post-human relationships and exchanges. This includes an entire chain encompassing infrastructure, architectures, actors and their practices, but is more than its constituent parts. Thus, what is emerges as ethics is a culture around the maintenance, role and regulation of artificial intelligence in society.\n\nThere are 48 synonyms for error according to the Roget’s English Thesaurus. Error, as a category, is as big as, and keeps defining, its opposite, which is, perhaps, not even an opposite, but is more like another part of. Error is a twin, the Cane to the Abel of accuracy and optimisation. Rather than cast error out, or write it off, I want to write it in, and not just as a shadow, or in invisible ink, as a footnote, or awkward afterthought.\n\nLucy Suchman is a feminist theoretician who thinks about what it means to be with and alongside technologies. She asks about “the relation between cultural imaginaries -that is, the kind of collective resources we have to think about the world – and material practices. How are those joined together?” (2013). In that vein I want to think about what it means to be in close relationships and working with machines that, in a sense, rely on human judgment and control for optimisation.\n\nI believe it may be important to think through error differently because of how increasingly complex it is to think about responsibility and accountability in quantified systems that are artificially intelligenti. How do you assign accountability for errors in complex, dynamic, multi-agent technical systems?\n\nTake the case of the recent Tesla crash, the first death of a human being in a driverless car context. In May 2016, an ex-US Navy veteran was driving a car and watching a Harry Potter movie at the same time. The man was a test driver for a Tesla semi-autonomous car in autopilot mode. The car drove into a long trailer truck whose height and white surface was misread by the software for the sky. The fault, it seemed, was the driver’s for trusting the auto-pilot mode. The company’s condolence statement clarifies the nature of auto-pilot (Tesla 2016):\n\nWhen drivers activate Autopilot, the acknowledgment box explains, among other things, that Autopilot “is an assist feature that requires you to keep your hands on the steering wheel at all times,” and that “you need to maintain control and responsibility for your vehicle” while using it. Additionally, every time that Autopilot is engaged, the car reminds the driver to “Always keep your hands on the wheel. Be prepared to take over at any time.” The system also makes frequent checks to ensure that the driver’s hands remain on the wheel and provides visual and audible alerts if hands-on is not detected. It then gradually slows down the car until hands-on is detected again.\n\nHerein lies a key idea that runs like a deep vein through the history of machine intelligence: that machines are more accurate and better than humans in a wide variety of mechanical and computational tasks, but that humans must have overall control and responsibility because of their (our) superior abilities, because of something ephemeral, disputed, and specific that we believe makes us different. Yet, we are allowed to, and even expected to, make mistakes.\n\nFor machines, error comes down to design and engineering, at least according to Google. Early in its history the Google driverless car was a little too perfect for humans; it follows the rules perfectly – exactly what they are programmed to do. Humans however, break the rules: they make mistakes, take short cuts, and break rules (Naughton, 2015):\n\nGoogle is working to make the vehicles more “aggressive” like humans — law-abiding, safe humans — so they “can naturally fit into the traffic flow, and other people understand what we’re doing and why we’re doing it,” Dolgov said. “Driving is a social game.”“It’s a sticky area,” Schoettle said. “If you program them to not follow the law, how much do you let them break the law?\n\nThe Tesla crash outcome follows a certain historical continuity. American scholars Madeleine Elish and Tim Hwang (2014) show that in the history of cars and driving in America, human error tends to be cited as the most common reason for accidents; the machine is not flawed, it is human error in managing the machine. In the 1920s-30s when a number of crashes occurred, ‘reckless driving’ rather than poor design (of which there was a lot back then) was blamed for accidents (Leonardi 2010) There has been a tendency to “praise the machine and punish the human” say Elish and Hwang. So, the machine is assumed to be smart but not responsible, capable but not accountable; they are “almost minds” as Donna Haraway famously said of children, AI computer programs and non-human primates (1985).\n\nOne of the other ways in which error and accountability are being framed can be understood through the deployment of the “Trolley Problem” as an ethical standard for driverless car technology. In this, responsibility for accuracy and errors is seen to lie with software programming. The Trolley Problem thus also determines what appropriate driving is in a way that has never quite been outlined for human drivers.\n\nThe Trolley Problem is a classic thought experiment developed by the Oxford philosopher, Philippa Foot (originally to discuss the permissibility of abortion). The Trolley problem is presented as a series of hypothetical, inevitably catastrophic situations in which consequentialist (or, teleological) versus deontological ethics must be reconciled in order to select the lesser of two catastrophes. In the event of catastrophe, should more people be saved, or should the most valuable people be saved? In short: how can one human life be valued over another?\n\nMaking this difficult decision is presented as what artificial intelligence will have to achieve before driverless cars can be considered safe for roads; the problem is that software have not yet been programmed to tackle this challenge. If machine learning intelligence is to be relied on to solve this problem, it first needs a big enough training database to learn from. Such a training database of outcomes from various work-throughs of the Trolley Problem have not been made. Initiatives such as MIT’s new Moral Machine project are possibly building a training database of human- level scenarios for appropriate action.\n\nHowever, the Trolley Problem has since fallen out of favour in discussing ethics and driverless cars (Davis 2015). Scholars such as Vikram Bhargava, working with the scholar Patrick Lin, have already identified limitations in the Trolley Problem and are seeking more sophisticated approaches to programming decision-making in driverless cars (2016). The Trolley Problem, and other ethical tests based on logical reasoning, has been one of the ways in which ethics has been framed: first, as a mathematical problem, and second, as something that lends itself to software programming.\n\nThere has been a call to look at the contexts of production of technology for greater transparency and understanding of how AI will work in the world (Crawford, 2016; Elish and Hwang 2016). Diane Vaughn’s landmark investigation and analysis of the 1986 Challenger space shuttle tragedy gives us some indications of what the inside of technology production looks like in the context of a significant error. In this, Vaughn names the normalisation of deviance as the culprit for the design flaw, rather than malafide intent (Vaughn, 1997).\n\nThe ‘normalisation of deviance‘ refers to a slow and gradual loosening of standards for the evaluation and acceptance of risk in an engineering context. The O rings on the rocket boosters of Challenger that broke on that unusually cold January morning in Cape Canaveral, Florida, did so despite considerable evidence of its questionable performance in low temperature conditions. The space shuttle’s launch date was also repeatedly delayed for this very reason. Yet, in what is possibly one of the best resourced space research organisations, NASA, how was this vital information overlooked? The normalisation of deviance is as much an organisational-cultural issue as it is about the technical details. Vaughan’s detailed ethnography of the managerial, technical and organisational issues that led up to the Challenger disaster presents a valuable precedence and inspiration for the study of high-end technology production cultures and how errors, crises and mistakes are managed within engineering.\n\nDesign or use-case? Intuition or bureaucracy? Individual or organisation? The sites of being and error-ing only multiply.\n\nThis ethnography of error comes up against a planetary scale error that queers the pitch. Australia is located on tectonic plates that are moving seven centimetres north year; so, the whole country will move by five feet this year. This may not mean much for human geography but it means something for the shadow world of machine-readable geography: maps used by driverless cars, or driverless farm tractors, are now going to have inexact data to work from (Manaugh 2016). It’s difficult to say how responsibility will be assigned for errors resulting from this shift.\n\nReferences\n\nBhargava, V (forthcoming) What if Blaise Pascal designed driverless cars? Towards Pascalian Autonomous Vehicles. in Patrick Lin, George Bekey, Keith Abney, and Ryan Jenkins (Eds.), Roboethics 2.0. MIT Press.\n\nCrawford, K (2016) Artificial Intelligence’s White Guy Problem. The New York Times. http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=0 retrieved July 25, 2016\n\nCrawford, K. and Whittaker, M (2016). The Social and Economic Implications of Artificial Intelligence Technologies in the Near-Term . Symposium report. https://artificialintelligencenow.com/media/documents/AINowSummaryReport_3.pdf Retrieved October 2, 2016.\n\nDavis, L.C (2015) ‘Would you pull the trolley switch? Does it matter?’ Lauren Cassani Davis in The Atlantic, October 9, 2015. Retrieved October 1, 2016 http://www.theatlantic.com/technology/archive/2015/10/trolley-problem-history-psychology-morality-driverless-cars/409732/\n\nElish, M and Hwang, T (2014) Praise the machine! Punish the human! The contradictory history of accountability in automated aviation. Comparative Studies in Intelligent Systems – Working Paper #1 Intelligence and Autonomy Initiative1. February 24 2015. Data & Society. Accessed http://www.datasociety.net/pubs/ia/Elish-Hwang_AccountabilityAutomatedAviation.pdf Retrieved September 23, 2015.\n\nElish and Hwang (2016) An AI Pattern Language Published by the Intelligence & Autonomy Initiative of Data & Society. http://autonomy.datasociety.net/patternlanguage/ Retrieved October 5, 2016\n\nFoot, P (1967) The Problem of Abortion and the Doctrine of the Double Effect. Oxford Review, No. 5. Included in Foot, 1977/2002 Virtues and Vices and Other Essays in Moral Philosophy.\n\nHaraway, D (1990) Primate Visions: Gender, race and nature in the world of modern science. Routledge\n\nLeonardi, P (2010) From Road to Lab to Math: The Co-evolution of Technological,Regulatory,and Organizational Innovations forAutomotive Crash Testing. Social Studies of Science 40/2; 243–274.\n\nManaugh, G (2016) Plate Tectonics Affects How Robots Navigate. Motherboard http://motherboard.vice.com/en_uk/read/plate-tectonics-gps-navigation retrieved October 2, 2016\n\nOrlikowski, W.J (2000) Using Technology and Constituting Structures: A Practice Lens for Studying Technology inOrganizations. Organization Science, Vol. 11, No. 4 (Jul. – Aug., 2000), pp. 404-428.\n\nNaughton, K (2015) Humans Are Slamming Into Driverless Cars and Exposing a Key Flaw, Bloomberg Technology News, December 17, 2015: https://www.bloomberg.com/news/articles/2015-12-18/humans-are-slamming-into-driverless-cars-and-exposing-a-key-flaw retrieved February 5, 2016\n\nSpector, M (2016) ‘Obama Administration Rolls Out Recommendations for Driverless Cars’, Wall Street Journal, September 191, 2016. http://www.wsj.com/articles/obama-administration-rolls-out-recommendations-for-driverless-cars-1474329603 Retrieved October 1, 2016\n\nSuchman, L (2013) Traversing technologies: Feminist research at the digital/material boundary. From video and transcript of a talk at the University of Toronto at the colloquia series Feminist and Queer Approaches to Technoscience: http://sfonline.barnard.edu/traversing-technologies/lucy-suchman-feminist-research-at-the-digitalmaterial-boundary/\n\nTesla (2016). A Tragic Loss. Blog post on Tesla website. https://www.teslamotors.com/blog/tragic-loss retrieved June 2016\n\nVaughan, D. (997) The Challenger launch decision: Risky technology, culture and deviance at NASA. University of Chicago.\ni I follow the definition of artificial intelligence proposed by Kate Crawford and Meredith Whittaker, that it is a “constellation of technologies comprising big data, machine learning and natural language processing” as described in the recent symposium AI Now: The Social and Economic Implications of Artificial Intelligence Technologies in the Near-Term. Symposium report available here: https://artificialintelligencenow.com/media/documents/AINowSummaryReport_3.pdf Retrieved October 2, 2016.\n",
    "longIndex": 0,
    "nbOfLong": 1,
    "txtshort": "This text is an offshoot of a period of research on an obsolete network protocol, X.25, which was taken as a technical object of comparison to study the ways in which contemporary Internetworking works. Being informed by particular hands-on experiences with protocols, it is written as an accompanying text for a small practical exercise.\nLegacy\n\nIn the early days of internetworking, before the establishment of the Internet Protocol suite (TCP/IP) as the global undisputed standard, different protocols were developed in competition with one another. X.25 was one such competitor and was for more than two decades the hegemonic networking technology. X.25 and TCP/IP emerged around the same period in response to similar questions, and still co-exist nowadays, representing alternative networking paradigms. However, X.25 is used in few specific cases and is been phased out for many years, hence it is often described as a legacy protocol in technical literature.\nLegacy in this context is used as an adjective to refer to methods, software or hardware considered obsolete. The term is used to derogatorily point at the burden of maintaining older components of a system which can not be removed without breaking support for or compatibility with older versions, thus requiring extra work and care. Outside of technical parlance, used as a noun, legacy is defined as an inheritance, or as the outcomes of past events.\nThe interplay between the technical parlance and the colloquial meaning of the word suggested a way to understand the long-term relations between systems that are decades apart from each other.\nThe text that follows is an attempt at activating the concept of legacy as an approach to look at and speak about technology. Legacy protocols, formats and systems are reluctantly dragged along into the present together with shiny black-boxed interfaces. This offers a continuity in which structures inherited from the past appear next to those of the present. As a result, legacies implicitly question current paradigms of innovation and progress and return technological artifacts to their complex historical dimension.\nThe potential of these legacies comes from the way they can be experienced from within, through the practice of use, shifting our ordinary punctual relation to a technical object. Any such object always involves certain long-term elements, which can be mobilized to obtain a temporal displacement in the functioning of the object, and in our understanding of its functioning.\nLegacy is therefore a site to ground archaeological and genealogical approaches in the devices and systems we encounter in our everyday life.\nIn practice, it often implies anomaly, deviance or mis-functioning of the normal operation of something. Nonetheless, the attempted use of X.25 was in itself a revealing experience of some fundamental differences between the TCP/IP protocol and its legacy other.\nNetwork\n\nThe assumption that one could experience the X.25 by reading technical manuals and setting up a connection was dispelled in understanding the complex interdependence between the protocol and the infrastructure it was meant to run on. Our understanding of the functioning of a protocol is influenced by our existing knowledge of the TCP/IP, so we just tried to apply the way we would relate to that. What we didn’t know, is how much the design choices of TCP/IP were fundamentally taken in opposition to the paradigms of networks like X.25.\nAt the time of the parallel development of the two protocols, the debate on internetworking was polarized along two constellations of agents, interests, industries and technologies.\nThis debate can be re-casted by accessing the Oral Histories of the Internet, told by the ‘protagonists’ of this history, many of whom are still alive, and most of which are involved in reconstructing an own personal account of the events. As a result one has to be wary that these individual histories are biased in many ways ranging from the heroic memories of the pioneers to the accounts of the ones that didn’t make it into the Internet Hall of Fame but want to reinstate their participation in the process. They are still an invaluable resource to try and track the evolution of the technical and political controversies.\nIn most Oral Histories, the two constellations are generally recalled in a stylized fashion, and we will idly rehash them once again for simplicity. On one side X.25 was promoted by the national telephone monopolies trying to enter the nascent market of computer networking by applying their knowledge of networks, thus protecting their interests in the existing infrastructures. On the other, TCP/IP was funded by the US Department of Defense and supported by the emerging computer industries, stressing a model in which the terms for computer networking would not be dictated by the PTTs(Post, Telegraph and Telephone companies) and where computer networking could be built up from the start.\nWhile presented as opposed, both these formations worked on experimental methods of interconnecting computers over long distances, however they differed in methods and priorities based on different requirements and economies. The predominant way that this contraposition can be read in the histories of the Internet is of a clash between computing culture and telephony culture. As a sub-text, the telephony world represents the status-quo and old way of doing things, based on national monopolies, their hierarchical and bureaucratic proceedings, their concerns for the wellbeing of national industries and technological sovereignty and therefore their political interference with the markets. On the other hand the emerging world of the computer industries and internetworking is depicted as based on (smaller scale) private enterprise, transnational collaboration, flat hierarchies and designs based on pure technical merit as opposed to any form of politics.\nThis account sounds suspicious today, for its linearity, its partitioning between political and apolitical, its accounts of market freedom, or the unclear role of the US Department of Defense, to name a few. Still, accepting this narrative as integral part of the legacy, offers an interesting angle to analyze the mutually productive relation between technical design choices and ideological positions, rather than limit ourselves to deconstruct the latter.\nBearing in mind this loose sketch of urgencies, agents and discourse around the development of the two paradigms, we can turn now to their conceptual and technical oppositions that partake in the diagram.\nParadigm\n\nBoth IP and X.25 are protocols for so-called packet switched networks. Packet switching is a technique to split up data into separate small chunks (packets) before transmission, so multiple connections can simultaneously travel over the same line. On arrival the packets are reassembled and the data reconstructed. The two differ most prominently in the understanding of how this packet switching should happen. The two approaches proposed were a connection-oriented paradigm called Virtual Circuit and a connectionless paradigm called Datagram.\n\nX.25 has been the most important attempt based on the Virtual Circuit model of packet switching. The Virtual Circuit was proposed as a commercially viable method of using the existing infrastructure, unsurprisingly owned by the same parties that were funding the development. For this reason they imagined the existing and widespread telephone system as the basis for future data networks. In their model the switching through the network (routing) was handled by management centers. The management meant ensuring that all packets in the transmission flowed over the same route and arrive in the correct order. So it was the equipment of the telephone network, rather than the one of the users, that would take care of correct transmission of data and the allocation of bandwidth for the connection. The operation of dedicated leased lines would be guaranteed by the PTTs, which would charge a fee for this service and retain a role of central management of the flows in the network.\n\nThe paradigm on which IP is modeled, instead, is the connectionless model named Datagram, that came out of experimental networking research. This model reconceptualized the network as an agglomeration of separated and interconnected computer networks. The packets, or datagrams are created embedding their own addressing information, that the devices in the network use to forward packets to the next device, repeating this process until the destination is reached. It is then the sending and receiving machines at the ends of the connection that are responsible for tasks such as ensuring delivery, order and timeout of packets. This allowed networks to be imagined without facing the status of the existing infrastructure, fictionalizing it as a series of ducts through which the data would flow without the network having to care for it, an approach dubbed network agnostic. This approach implied that the computer industry would be the main influence on computer networking while the telephone companies should just provide the conduits for it.\nWhat was sketched above are just the conceptual models that guided the development of the protocols, the development of both meant actually an hybridization of the two models. In the case of the datagram-based IP, for example, the diffusion and success came by coupling it with the companion protocol TCP, which is itself a Virtual Circuit protocol. The Internet’s TCP/IP set is thus an application of both approaches.\n\nIn synthesis, one main contraposition between the two conceptual models was in the way in which they related to the network infrastructure. The Virtual Circuit model implied that the physical infrastructure was the most important element of the network, whereas the Datagram model proposed an abstraction of the logical network from its material grounding, implying that it doesn’t (or shouldn’t) matter. Even though both protocols had to compromise with the material conditions of the network, and both ended up as a hybrid of the two models, the debate on the two opposing conceptions of internetworking achieved an own agency, which still retains an influence today.\nNarrative\n\nThe legacy of the contrapositions that were outlined in the previous sections is evident in a number of contemporary debates. A clear example is the controversy over the prospect of management of certain flows of data over the Internet, which has been catalyzed by the concept of net neutrality. Net neutrality is assumed as an essential quality of the Internet, which has accompanied its development and should be protected. The general discourse, championed by digital rights organizations such as EFF ( https://www.eff.org/issues/net-neutrality ) and by companies such as Google ( https://www.google.com/takeaction/action/freeandopen/index.html ), is that a network should not discriminate the way that information is vehicled through on the base of its content, if at all.\nLeaving aside the extent to which certain types of management have already been in place for some time, the source of this concept as a foundational quality of the Internet can be tracked back to the debate that was just outlined. The idea of net neutrality is closely related to the idea of the dumb pipe, circulating in the TCP/IP formation, which fictionalizes a complete separation between the physical and logical levels of communication. It is generally taken to mean the localization of intelligence, in the sense of the interpretation of content, at the ends of the network, while the in-between flow of information happen through a network of neutral ducts.\nThe political significance of such design principles kept implicit in the early times of the debate, which pretended to be limited to a technical confrontation among models and topologies. It was evident though to all the people involved what economies and world-views the two paradigms represented, due to manifest elements such as who was funding the research and pushing for adoption.\nIt is with the prevailing of the TCP/IP protocol and the global success of the Internet that discourse sedimented, equating some successful design choices of the protocol with certain ethical and political values.\nA representative example is the 1998 article “Rise of the Stupid Network” by David Isenberg that describes how “the Internet, because it makes the details of network operation irrelevant, is shifting control to the end user”, and “end user devices would be free to behave flexibly because, in the Stupid Network the data is boss, bits are essentially free.” So “the Internet that we know and love is a “virtual network” – a “network of networks” – that is independent of wires and transport protocols.”\nWhat the legacy of the DG/VC debate suggests is that these values should be read as traits that the TCP/IP protocol had at specific point in time, as a reaction to a competitor with another set of features, rather than as intrinsic qualities of TCP/IP themselves. It is therefore ironical but not surprising that the dissemination of these Internet narratives happened concurrently to the revision of the design choices to which these inherent qualities were ascribed, and that the reason for such compromises was precisely the increasing popularity of TCP/IP. The success of the protocol introduced material issues of scaling, of composition with the existing infrastructure and negotiation with its related economies.\n\n\nThe text will continue with/informed by the proceedings of a practical workshop that attempts to probe some elements of the current formation of the Internet, in tension with the legacy outlined above.\nReferences\n\n    Abbate, Janet. Inventing the Internet. 58839th edition. The MIT Press, 2000.\n    Bowker, Geoffrey C., Karen Baker, Florence Millerand, and David Ribes. “Toward Information Infrastructure Studies: Ways of Knowing in a Networked Environment.” In International Handbook of Internet Research, edited by Jeremy Hunsinger, Lisbeth Klastrup, and Matthew Allen, 97–117. Springer Netherlands, 2009.\n    cheek, cris, Braxton Soderman and Nicole Starosielski. “Network Archaeology,” Amodern Journal. Vol. 2 (Fall 2013).\n    DeNardis, Laura. Protocol Politics: The Globalization of Internet Governance. The MIT Press, 2014.\n    Després, Rémi. Oral history interview with Rémi Després by Valérie Schafer. Oral History, May 16, 2012. http://conservancy.umn.edu/handle/11299/155671.\n    ———. “X.25 Virtual Circuits – TRANSPAC IN France – Pre-Internet Data Networking.” IEEE Communications Magazine 48, no. 11 (November 2010): 40–46.\n    Galloway, Alexander R. Protocol: How Control Exists after Decentralization. MIT Press, 2004.\n    Pelkey, James. “CYCLADES Network and Louis Pouzin 1971 – 1972,” 2007. http://www.historyofcomputercommunications.info/Book/6/6.3-CYCLADESNetworkLouisPouzin1-72.html.\n    Pouzin, Louis. Oral history interview with Louis Pouzin by Andrew L. Russell. Oral History, April 2, 2012. http://conservancy.umn.edu/handle/11299/155666.\n    ———. “Virtual Circuits vs. Datagrams: Technical and Political Problems,” 483–94. ACM, 1976. doi:10.1145/1499799.1499870.\n    Russell, Andrew L. Open Standards and the Digital Age: History, Ideology, and Networks. Cambridge University Press, 2014.\n",
    "shortIndex": 0,
    "nbOfShort": 2,
    "fontwords": [],
    "black": true
}